{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similaridade de texto (palavras)\n",
    "\n",
    "A similaridade de texto é utilizada para comparar palavras similares (que foram escritas de forma diferente).\n",
    "Alguns exemplos:\n",
    "\n",
    "* `São Paulo`\n",
    "* `Sao Paulo`\n",
    "*  `São Paulo   ` (tem espaço no final)\n",
    "* `são paulo`\n",
    "\n",
    "Todas as palavras acima se referem a cidade de São Paulo, mas se efetuar umas simples comparação de _strings_ (`str1 == str2`) o resultado será `False`.\n",
    "\n",
    "É possível tratar esses casos, como (`str.strip()`) para remover espaços no começo/final do texto.\n",
    "Outro detalhe é `str.lower()` ou `str.upper()` para manter tudo minúsculo/maiúsculo.\n",
    "Finalmente, é possível remover toda a acentuação para evitar problemas.\n",
    "\n",
    "Entretanto, essa não é uma boa adordagem (principalmente remover acentuação, as outras duas podem ser boas ideias) visto que estamos alterando o texto para encontrar o nome `São Paulo`.\n",
    "\n",
    "Uma alternativa é computar um número (entre 0 e 100, por exemplo) que indica o quão similares são duas palavras.\n",
    "Sendo que, 0 indica palavras completamente diferentes e 100 indica que as palavras são idênticas.\n",
    "Assim, podemos definir um limiar de forma que, qualquer similaridade acima de 80 aceitamos como algo válido.\n",
    "O valor de limiar vai variar com o contexto, por isso é importante testar várias abordagens.\n",
    "\n",
    "Existem várias métricas para calcular essa similaridade: https://en.wikipedia.org/wiki/Edit_distance\n",
    "\n",
    "Vamos trabalhar com a distância de Levenshtein: https://en.wikipedia.org/wiki/Levenshtein_distance\n",
    "\n",
    "A distância de Levenshtein calcula o número minímino de alterações (inserir, remover, substituir) que devemos fazer em uma string para que ela fique igual a outra.\n",
    "\n",
    "Tutorial `fuzzywuzzy` https://www.datacamp.com/community/tutorials/fuzzy-string-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julio/Projects/pos-furb-recuperacao/.venv/lib/python3.6/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'Sao Paulo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'sao paulo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'São Paulo    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('Sao Paulo', 'São Paulo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, podemos ter uma lista de cidades e ver quais pessoas moram em `São Paulo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 90, 89, 67, 82, 82, 24, 24, 35]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pessoas = [\n",
    "    {'nome': 'Pessoa A', 'cidade': 'São Paulo'},\n",
    "    {'nome': 'Pessoa B', 'cidade': ' São Paulo '},\n",
    "    {'nome': 'Pessoa C', 'cidade': 'Sao Paulo'},\n",
    "    {'nome': 'Pessoa D', 'cidade': 'sao paulo'},\n",
    "    {'nome': 'Pessoa E', 'cidade': 'SaoPaulo'},\n",
    "    {'nome': 'Pessoa F', 'cidade': 'S. Paulo'},\n",
    "    {'nome': 'Pessoa G', 'cidade': 'Blumenau'},\n",
    "    {'nome': 'Pessoa H', 'cidade': 'Pomerode'},\n",
    "    {'nome': 'Pessoa I', 'cidade': 'Rio de Janeiro'},\n",
    "]\n",
    "\n",
    "scores = [fuzz.ratio('São Paulo', p['cidade']) for p in pessoas]\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entretanto, nem sempre é possível comparar o texto diretamente.\n",
    "Por exemplo, o nome da cidade pode estar no meio de uma frase.\n",
    "Para isso, é possível fazer uma comparação parcial.\n",
    "\n",
    "`fuzz.ratio()` compara dois textos por completo, com a similaridade entre eles.\n",
    "\n",
    "`fuzz.partial_ratio()` faz a comparação em substrings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.ratio('São Paulo', 'João mudou-se para São Paulo em 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('São Paulo', 'João mudou-se para São Paulo em 2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('São Paulo', 'João mudou-se para Sao Paulo em 2019')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro caso comum, é a ordem dos textos.\n",
    "\n",
    "Por exemplo, `Comparamos o Dispositivo A com o Dispositivo B` e `Comparamos o Dispositivo B com o Dispositivo A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('Dispositivo A com Dispositivo B', 'Comparamos Dispositivo A com Dispositivo B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('Dispositivo A com Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio('Dispositivo A com Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro caso é o uso da comparação `Dispositivo A com Dispositivo B` ou `Dispositivo A x Dispositivo B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio('Dispositivo A x Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_set_ratio('Dispositivo A x Dispositivo B', 'Comparamos dispositivo B com dispositivo A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento de linguagem natural\n",
    "\n",
    "São técnicas utilizadas para trabalhar com texto que englobam desde _tokenização_ (separar uma string em palavras) até a tradução de texto.\n",
    "\n",
    "A similaridade de palavras acima demonstra um uso de processamento de linguagem natural.\n",
    "\n",
    "Nesta parte, vamos ver alguns conceitos básicos utilizando as bibliotecas `NLTK` e `spaCy`.\n",
    "\n",
    "Uma comparação entre `NLTK` e `spaCy` https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonte http://www.furb.br/web/1704/noticias/furb-esta-entre-4-melhores-de-sc-em-ranking-da-america-latina/7944\n",
    "texto = '''O Brasil desponta com as melhores Universidades da América Latina, com o total de 3 no top 10,\n",
    "seguida do Chile, Colômbia e México com 2 em cada país, além da Argentina com uma.\n",
    "No primeiro lugar está a Pontifícia Universidade Católica do Chile, seguida da Universidade de São Paulo (USP).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "Separara um texto em uma lista de palavras.\n",
    "Note que a forma mais simples de resolver esse problema é `texto.split(' ')` apenas separando por espaços.\n",
    "Entretanto, essa abordagem falha em um texto como `... fim do dia.`, onde `dia.` seria um token, mas deveria ser separdo em dois: `dia` e `.`.\n",
    "O mesmo acontece para nomes, `João B. da Silva`, é importante notar que `B.` não é o final da frase, mas uma abreviação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pode ser necessário executar\n",
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "Brasil\n",
      "desponta\n",
      "com\n",
      "as\n",
      "melhores\n",
      "Universidades\n",
      "da\n",
      "América\n",
      "Latina\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(texto)\n",
    "for (i, token) in enumerate(tokens):\n",
    "    print(token)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode ser necessário instalar o modelo\n",
    "# python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\n",
      "Brasil\n",
      "desponta\n",
      "com\n",
      "as\n",
      "melhores\n",
      "Universidades\n",
      "da\n",
      "América\n",
      "Latina\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "for (i, token) in enumerate(doc):\n",
    "    print(token.text)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['desponta', 'top', 'seguida', 'está', 'seguida']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apenas os verbos\n",
    "verbos = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "verbos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de _stopwords_\n",
    "\n",
    "_Stopwords_ são palavras utilizadas para conectar texto (`de`, `a`, `e`, outros) e não estão ligadas diretamente ao sentido da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode ser necessário executar\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'a', 'o', 'que', 'e', 'é', 'do', 'da', 'em', 'um']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sws = stopwords.words('portuguese')\n",
    "sws[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'Brasil',\n",
       " 'desponta',\n",
       " 'melhores',\n",
       " 'Universidades',\n",
       " 'América',\n",
       " 'Latina',\n",
       " ',',\n",
       " 'total',\n",
       " '3']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_sem_stopwords = [t for t in tokens if t not in sws]\n",
    "print(len(tokens), len(tokens_sem_stopwords))\n",
    "tokens_sem_stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[O, Brasil, desponta, melhores, Universidades, América, Latina, ,, o, total]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokens_sem_stopwords = [t for t in doc if not t.is_stop]\n",
    "spacy_tokens_sem_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem\n",
    "\n",
    "_Stem_ é um método para reduzir uma palavra ao radical/raiz.\n",
    "Como os verbos são conjugados, pode ser difícil trabalhar com todas as variações.\n",
    "Uma forma de reduzir esse problema, é trabalhar apenas com o radical do verbo.\n",
    "Por: `copiar` -> `copi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pode ser necessário executar\n",
    "# import nltk\n",
    "# nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'necess'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"necessário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'necess'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"necessidade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'desnecess'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"desnecessário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " 'brasil',\n",
       " 'despont',\n",
       " 'melhor',\n",
       " 'univers',\n",
       " 'amér',\n",
       " 'latin',\n",
       " ',',\n",
       " 'total',\n",
       " '3']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_stem = [stemmer.stem(t) for t in tokens_sem_stopwords]\n",
    "tokens_stem[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Enquanto _stemming_ reduz as palavras removendo partes no começo e final, _lemmatization_ reduz a palavra na forma do dicionário.\n",
    "\n",
    "Para mais informações: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'Brasil',\n",
       " 'despontar',\n",
       " 'melhorar',\n",
       " 'Universidades',\n",
       " 'América',\n",
       " 'Latina',\n",
       " ',',\n",
       " 'o',\n",
       " 'total']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = [t.lemma_ for t in spacy_tokens_sem_stopwords]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) tagging\n",
    "\n",
    "Anota as palavras de acordo com a sua função linguística na frase.\n",
    "Ou seja, indica se uma palavra é um verbo, substantivo, pontuação, outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET',\n",
       " 'PROPN',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PROPN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = [t.pos_ for t in spacy_tokens_sem_stopwords]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desponta\n",
      "top\n",
      "seguida\n",
      "está\n",
      "seguida\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas os verbos\n",
    "for token in filter(lambda x: x.pos_ == 'VERB', doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total\n",
      "país\n",
      "lugar\n",
      "Pontifícia\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas os substantivos\n",
    "for token in filter(lambda x: x.pos_ == 'NOUN', doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil\n",
      "Universidades\n",
      "América\n",
      "Latina\n",
      "10\n",
      "do\n",
      "Chile\n",
      "Colômbia\n",
      "México\n",
      "Argentina\n",
      "Universidade\n",
      "Católica\n",
      "Chile\n",
      "da\n",
      "Universidade\n",
      "São\n",
      "Paulo\n",
      "USP\n"
     ]
    }
   ],
   "source": [
    "# pegando apenas os nomes próprios\n",
    "for token in filter(lambda x: x.pos_ == 'PROPN', doc):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "\n",
    "Reconhece e classifica entidades no texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil LOC\n",
      "Universidades da América Latina PER\n",
      "Chile LOC\n",
      "Colômbia LOC\n",
      "México LOC\n",
      "Argentina LOC\n",
      "Pontifícia Universidade Católica do Chile LOC\n",
      "Universidade de São Paulo LOC\n",
      "USP LOC\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
